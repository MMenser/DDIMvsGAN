{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNL3LEm+oPXDcsdNIVb6SzZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MMenser/DDIMvsGAN/blob/main/mason/DDIMMnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BZ8pyw1-QgnR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import math\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from datetime import datetime\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "\n",
        "# ============================================================\n",
        "# DATASET SETTINGS\n",
        "# ============================================================\n",
        "dataset_name = \"mnist\"\n",
        "num_epochs = 50\n",
        "image_size = 64\n",
        "\n",
        "# MNIST → 28×28 grayscale → upscale to 64×64 grayscale (or RGB if needed)\n",
        "# If your DDIM network expects 3-channel images, we convert 1 → 3 channels.\n",
        "\n",
        "# KID settings (leave unchanged)\n",
        "kid_image_size = 299\n",
        "kid_diffusion_steps = 20\n",
        "plot_diffusion_steps = 20\n",
        "\n",
        "# Sampling\n",
        "min_signal_rate = 0.02\n",
        "max_signal_rate = 0.95\n",
        "\n",
        "# Architecture\n",
        "embedding_dims = 32\n",
        "embedding_max_frequency = 1000.0\n",
        "widths = [32, 64, 96, 128]\n",
        "block_depth = 2\n",
        "\n",
        "# Optimization\n",
        "batch_size = 64\n",
        "ema = 0.999\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 1e-4\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# PREPROCESSING FOR MNIST\n",
        "# ============================================================\n",
        "def preprocess_image(data):\n",
        "    # MNIST gives image as shape (28, 28, 1), uint8\n",
        "    image = tf.cast(data[\"image\"], tf.float32) / 255.0\n",
        "\n",
        "    # Resize to your model size (64x64)\n",
        "    image = tf.image.resize(image, size=[image_size, image_size], antialias=True)\n",
        "\n",
        "    # If model expects 3 channels, convert grayscale → RGB\n",
        "    if image.shape[-1] == 1:\n",
        "        image = tf.image.grayscale_to_rgb(image)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# DATASET LOADER\n",
        "# ============================================================\n",
        "def prepare_dataset(split):\n",
        "    return (\n",
        "        tfds.load(dataset_name, split=split, shuffle_files=True)\n",
        "        .map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        .cache()\n",
        "        .shuffle(10 * batch_size)\n",
        "        .batch(batch_size, drop_remainder=True)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "\n",
        "\n",
        "# MNIST does not have train/validation/test subsets like flowers; adjust splits:\n",
        "train_dataset = prepare_dataset(\"train\")\n",
        "val_dataset = prepare_dataset(\"test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MetricsLogger(keras.callbacks.Callback):\n",
        "    def __init__(self, filepath='training_metrics.csv'):\n",
        "        super().__init__()\n",
        "        self.filepath = filepath\n",
        "        self.metrics_history = []\n",
        "        self.epoch_start_time = None\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        import time\n",
        "        self.epoch_start_time = time.time()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        import time\n",
        "        import csv\n",
        "\n",
        "        logs = logs or {}\n",
        "\n",
        "        # Calculate epoch duration\n",
        "        epoch_time = time.time() - self.epoch_start_time if self.epoch_start_time else None\n",
        "\n",
        "        # Save metrics for this epoch\n",
        "        epoch_metrics = {'epoch': epoch}\n",
        "        if epoch_time is not None:\n",
        "            epoch_metrics['epoch_time'] = epoch_time\n",
        "        epoch_metrics.update(logs)\n",
        "        self.metrics_history.append(epoch_metrics)\n",
        "\n",
        "        # Write to CSV\n",
        "        with open(self.filepath, 'w', newline='') as f:\n",
        "            if self.metrics_history:\n",
        "                writer = csv.DictWriter(f, fieldnames=self.metrics_history[0].keys())\n",
        "                writer.writeheader()\n",
        "                writer.writerows(self.metrics_history)\n",
        "\n",
        "        print(f\"Metrics saved to {self.filepath} (Epoch time: {epoch_time:.2f}s)\")"
      ],
      "metadata": {
        "id": "kiGNeZv8UXIg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "def sinusoidal_embedding(x):\n",
        "    embedding_min_frequency = 1.0\n",
        "    frequencies = ops.exp(\n",
        "        ops.linspace(\n",
        "            ops.log(embedding_min_frequency),\n",
        "            ops.log(embedding_max_frequency),\n",
        "            embedding_dims // 2,\n",
        "        )\n",
        "    )\n",
        "    angular_speeds = ops.cast(2.0 * math.pi * frequencies, \"float32\")\n",
        "    embeddings = ops.concatenate(\n",
        "        [ops.sin(angular_speeds * x), ops.cos(angular_speeds * x)], axis=3\n",
        "    )\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "def ResidualBlock(width):\n",
        "    def apply(x):\n",
        "        input_width = x.shape[3]\n",
        "        if input_width == width:\n",
        "            residual = x\n",
        "        else:\n",
        "            residual = layers.Conv2D(width, kernel_size=1)(x)\n",
        "        x = layers.BatchNormalization(center=False, scale=False)(x)\n",
        "        x = layers.Conv2D(width, kernel_size=3, padding=\"same\", activation=\"swish\")(x)\n",
        "        x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n",
        "        x = layers.Add()([x, residual])\n",
        "        return x\n",
        "\n",
        "    return apply\n",
        "\n",
        "\n",
        "def DownBlock(width, block_depth):\n",
        "    def apply(x):\n",
        "        x, skips = x\n",
        "        for _ in range(block_depth):\n",
        "            x = ResidualBlock(width)(x)\n",
        "            skips.append(x)\n",
        "        x = layers.AveragePooling2D(pool_size=2)(x)\n",
        "        return x\n",
        "\n",
        "    return apply\n",
        "\n",
        "\n",
        "def UpBlock(width, block_depth):\n",
        "    def apply(x):\n",
        "        x, skips = x\n",
        "        x = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x)\n",
        "        for _ in range(block_depth):\n",
        "            x = layers.Concatenate()([x, skips.pop()])\n",
        "            x = ResidualBlock(width)(x)\n",
        "        return x\n",
        "\n",
        "    return apply\n",
        "\n",
        "\n",
        "def get_network(image_size, widths, block_depth):\n",
        "    noisy_images = keras.Input(shape=(image_size, image_size, 3))\n",
        "    noise_variances = keras.Input(shape=(1, 1, 1))\n",
        "\n",
        "    e = layers.Lambda(sinusoidal_embedding, output_shape=(1, 1, 32))(noise_variances)\n",
        "    e = layers.UpSampling2D(size=image_size, interpolation=\"nearest\")(e)\n",
        "\n",
        "    x = layers.Conv2D(widths[0], kernel_size=1)(noisy_images)\n",
        "    x = layers.Concatenate()([x, e])\n",
        "\n",
        "    skips = []\n",
        "    for width in widths[:-1]:\n",
        "        x = DownBlock(width, block_depth)([x, skips])\n",
        "\n",
        "    for _ in range(block_depth):\n",
        "        x = ResidualBlock(widths[-1])(x)\n",
        "\n",
        "    for width in reversed(widths[:-1]):\n",
        "        x = UpBlock(width, block_depth)([x, skips])\n",
        "\n",
        "    x = layers.Conv2D(3, kernel_size=1, kernel_initializer=\"zeros\")(x)\n",
        "\n",
        "    return keras.Model([noisy_images, noise_variances], x, name=\"residual_unet\")\n",
        "\n",
        "@keras.saving.register_keras_serializable()\n",
        "class DiffusionModel(keras.Model):\n",
        "    def __init__(self, image_size, widths, block_depth):\n",
        "        super().__init__()\n",
        "\n",
        "        self.normalizer = layers.Normalization()\n",
        "        self.network = get_network(image_size, widths, block_depth)\n",
        "        self.ema_network = keras.models.clone_model(self.network)\n",
        "\n",
        "    def compile(self, **kwargs):\n",
        "        super().compile(**kwargs)\n",
        "\n",
        "        self.noise_loss_tracker = keras.metrics.Mean(name=\"n_loss\")\n",
        "        self.image_loss_tracker = keras.metrics.Mean(name=\"i_loss\")\n",
        "        self.kid = KID(name=\"kid\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.noise_loss_tracker, self.image_loss_tracker, self.kid]\n",
        "\n",
        "    def denormalize(self, images):\n",
        "        # convert the pixel values back to 0-1 range\n",
        "        images = self.normalizer.mean + images * self.normalizer.variance**0.5\n",
        "        return ops.clip(images, 0.0, 1.0)\n",
        "\n",
        "    def diffusion_schedule(self, diffusion_times):\n",
        "        # diffusion times -> angles\n",
        "        start_angle = ops.cast(ops.arccos(max_signal_rate), \"float32\")\n",
        "        end_angle = ops.cast(ops.arccos(min_signal_rate), \"float32\")\n",
        "\n",
        "        diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\n",
        "\n",
        "        # angles -> signal and noise rates\n",
        "        signal_rates = ops.cos(diffusion_angles)\n",
        "        noise_rates = ops.sin(diffusion_angles)\n",
        "        # note that their squared sum is always: sin^2(x) + cos^2(x) = 1\n",
        "\n",
        "        return noise_rates, signal_rates\n",
        "\n",
        "    def denoise(self, noisy_images, noise_rates, signal_rates, training):\n",
        "        # the exponential moving average weights are used at evaluation\n",
        "        if training:\n",
        "            network = self.network\n",
        "        else:\n",
        "            network = self.ema_network\n",
        "\n",
        "        # predict noise component and calculate the image component using it\n",
        "        pred_noises = network([noisy_images, noise_rates**2], training=training)\n",
        "        pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates\n",
        "\n",
        "        return pred_noises, pred_images\n",
        "\n",
        "    def reverse_diffusion(self, initial_noise, diffusion_steps):\n",
        "        # reverse diffusion = sampling\n",
        "        startingTime = time.time()\n",
        "        num_images = initial_noise.shape[0]\n",
        "        step_size = 1.0 / diffusion_steps\n",
        "\n",
        "        # important line:\n",
        "        # at the first sampling step, the \"noisy image\" is pure noise\n",
        "        # but its signal rate is assumed to be nonzero (min_signal_rate)\n",
        "        next_noisy_images = initial_noise\n",
        "        for step in range(diffusion_steps):\n",
        "            noisy_images = next_noisy_images\n",
        "\n",
        "            # separate the current noisy image to its components\n",
        "            diffusion_times = ops.ones((num_images, 1, 1, 1)) - step * step_size\n",
        "            noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
        "            pred_noises, pred_images = self.denoise(\n",
        "                noisy_images, noise_rates, signal_rates, training=False\n",
        "            )\n",
        "            # network used in eval mode\n",
        "\n",
        "            # remix the predicted components using the next signal and noise rates\n",
        "            next_diffusion_times = diffusion_times - step_size\n",
        "            next_noise_rates, next_signal_rates = self.diffusion_schedule(\n",
        "                next_diffusion_times\n",
        "            )\n",
        "            next_noisy_images = (\n",
        "                next_signal_rates * pred_images + next_noise_rates * pred_noises\n",
        "            )\n",
        "            # this new noisy image will be used in the next step\n",
        "        print(f\"\\nSampling Time: {time.time() - startingTime}\")\n",
        "        return pred_images\n",
        "\n",
        "    def generate(self, num_images, diffusion_steps):\n",
        "        # noise -> images -> denormalized images\n",
        "        initial_noise = keras.random.normal(\n",
        "            shape=(num_images, image_size, image_size, 3)\n",
        "        )\n",
        "        generated_images = self.reverse_diffusion(initial_noise, diffusion_steps)\n",
        "        generated_images = self.denormalize(generated_images)\n",
        "        return generated_images\n",
        "\n",
        "    def train_step(self, images):\n",
        "          # normalize images to have standard deviation of 1, like the noises\n",
        "          images = self.normalizer(images, training=True)\n",
        "          noises = keras.random.normal(shape=(batch_size, image_size, image_size, 3))\n",
        "\n",
        "          # sample uniform random diffusion times\n",
        "          diffusion_times = keras.random.uniform(\n",
        "              shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
        "          )\n",
        "          noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
        "          # mix the images with noises accordingly\n",
        "          noisy_images = signal_rates * images + noise_rates * noises\n",
        "\n",
        "          with tf.GradientTape() as tape:\n",
        "              # train the network to separate noisy images to their components\n",
        "              pred_noises, pred_images = self.denoise(\n",
        "                  noisy_images, noise_rates, signal_rates, training=True\n",
        "              )\n",
        "\n",
        "              noise_loss = self.loss(noises, pred_noises)  # used for training\n",
        "              image_loss = self.loss(images, pred_images)  # only used as metric\n",
        "\n",
        "          gradients = tape.gradient(noise_loss, self.network.trainable_weights)\n",
        "          self.optimizer.apply_gradients(zip(gradients, self.network.trainable_weights))\n",
        "\n",
        "          self.noise_loss_tracker.update_state(noise_loss)\n",
        "          self.image_loss_tracker.update_state(image_loss)\n",
        "\n",
        "          # track the exponential moving averages of weights\n",
        "          for weight, ema_weight in zip(self.network.weights, self.ema_network.weights):\n",
        "              ema_weight.assign(ema * ema_weight + (1 - ema) * weight)\n",
        "\n",
        "          # KID is not measured during the training phase for computational efficiency\n",
        "          return {m.name: m.result() for m in self.metrics[:-1]}\n",
        "\n",
        "    def test_step(self, images):\n",
        "        # normalize images to have standard deviation of 1, like the noises\n",
        "        images = self.normalizer(images, training=False)\n",
        "        noises = keras.random.normal(shape=(batch_size, image_size, image_size, 3))\n",
        "\n",
        "        # sample uniform random diffusion times\n",
        "        diffusion_times = keras.random.uniform(\n",
        "            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
        "        )\n",
        "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
        "        # mix the images with noises accordingly\n",
        "        noisy_images = signal_rates * images + noise_rates * noises\n",
        "\n",
        "        # use the network to separate noisy images to their components\n",
        "        pred_noises, pred_images = self.denoise(\n",
        "            noisy_images, noise_rates, signal_rates, training=False\n",
        "        )\n",
        "\n",
        "        noise_loss = self.loss(noises, pred_noises)\n",
        "        image_loss = self.loss(images, pred_images)\n",
        "\n",
        "        self.image_loss_tracker.update_state(image_loss)\n",
        "        self.noise_loss_tracker.update_state(noise_loss)\n",
        "\n",
        "        # measure KID between real and generated images\n",
        "        # this is computationally demanding, kid_diffusion_steps has to be small\n",
        "        images = self.denormalize(images)\n",
        "        generated_images = self.generate(\n",
        "            num_images=batch_size, diffusion_steps=kid_diffusion_steps\n",
        "        )\n",
        "        self.kid.update_state(images, generated_images)\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def plot_images(self, epoch=None, logs=None, num_rows=4, num_cols=4):\n",
        "        # Create output directory if it doesn't exist\n",
        "        output_dir = \"generated_images\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Generate images\n",
        "        generated_images = self.generate(\n",
        "            num_images=num_rows * num_cols,\n",
        "            diffusion_steps=plot_diffusion_steps,\n",
        "        )\n",
        "\n",
        "        # Plot grid\n",
        "        plt.figure(figsize=(num_cols * 2.0, num_rows * 2.0))\n",
        "        for row in range(num_rows):\n",
        "            for col in range(num_cols):\n",
        "                index = row * num_cols + col\n",
        "                plt.subplot(num_rows, num_cols, index + 1)\n",
        "                plt.imshow(generated_images[index])\n",
        "                plt.axis(\"off\")\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save figure\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        filename = f\"epoch_{epoch:03d}_{timestamp}.png\" if epoch is not None else f\"sample_{timestamp}.png\"\n",
        "        filepath = os.path.join(output_dir, filename)\n",
        "        plt.savefig(filepath)\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"Saved generated images to {filepath}\")\n"
      ],
      "metadata": {
        "id": "Gw0iBQZgWA8R"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class KID(keras.metrics.Metric):\n",
        "    def __init__(self, name, **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "\n",
        "        # KID is estimated per batch and is averaged across batches\n",
        "        self.kid_tracker = keras.metrics.Mean(name=\"kid_tracker\")\n",
        "\n",
        "        # a pretrained InceptionV3 is used without its classification layer\n",
        "        # transform the pixel values to the 0-255 range, then use the same\n",
        "        # preprocessing as during pretraining\n",
        "        self.encoder = keras.Sequential(\n",
        "            [\n",
        "                keras.Input(shape=(image_size, image_size, 3)),\n",
        "                layers.Rescaling(255.0),\n",
        "                layers.Resizing(height=kid_image_size, width=kid_image_size),\n",
        "                layers.Lambda(keras.applications.inception_v3.preprocess_input),\n",
        "                keras.applications.InceptionV3(\n",
        "                    include_top=False,\n",
        "                    input_shape=(kid_image_size, kid_image_size, 3),\n",
        "                    weights=\"imagenet\",\n",
        "                ),\n",
        "                layers.GlobalAveragePooling2D(),\n",
        "            ],\n",
        "            name=\"inception_encoder\",\n",
        "        )\n",
        "\n",
        "    def polynomial_kernel(self, features_1, features_2):\n",
        "        feature_dimensions = ops.cast(ops.shape(features_1)[1], dtype=\"float32\")\n",
        "        return (\n",
        "            features_1 @ ops.transpose(features_2) / feature_dimensions + 1.0\n",
        "        ) ** 3.0\n",
        "\n",
        "    def update_state(self, real_images, generated_images, sample_weight=None):\n",
        "        real_features = self.encoder(real_images, training=False)\n",
        "        generated_features = self.encoder(generated_images, training=False)\n",
        "\n",
        "        # compute polynomial kernels using the two sets of features\n",
        "        kernel_real = self.polynomial_kernel(real_features, real_features)\n",
        "        kernel_generated = self.polynomial_kernel(\n",
        "            generated_features, generated_features\n",
        "        )\n",
        "        kernel_cross = self.polynomial_kernel(real_features, generated_features)\n",
        "\n",
        "        # estimate the squared maximum mean discrepancy using the average kernel values\n",
        "        batch_size = real_features.shape[0]\n",
        "        batch_size_f = ops.cast(batch_size, dtype=\"float32\")\n",
        "        mean_kernel_real = ops.sum(kernel_real * (1.0 - ops.eye(batch_size))) / (\n",
        "            batch_size_f * (batch_size_f - 1.0)\n",
        "        )\n",
        "        mean_kernel_generated = ops.sum(\n",
        "            kernel_generated * (1.0 - ops.eye(batch_size))\n",
        "        ) / (batch_size_f * (batch_size_f - 1.0))\n",
        "        mean_kernel_cross = ops.mean(kernel_cross)\n",
        "        kid = mean_kernel_real + mean_kernel_generated - 2.0 * mean_kernel_cross\n",
        "\n",
        "        # update the average KID estimate\n",
        "        self.kid_tracker.update_state(kid)\n",
        "\n",
        "    def result(self):\n",
        "        return self.kid_tracker.result()\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.kid_tracker.reset_state()\n"
      ],
      "metadata": {
        "id": "cVuuVMofVhCF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create and compile the model\n",
        "model = DiffusionModel(image_size, widths, block_depth)\n",
        "metrics_logger = MetricsLogger(filepath='training_metrics.csv')  # Add this line\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    ),\n",
        "    loss=keras.losses.mean_absolute_error,\n",
        ")\n",
        "# pixelwise mean absolute error is used as loss\n",
        "\n",
        "# save the best model based on the validation KID metric\n",
        "checkpoint_path = \"checkpoints/diffusion_model.weights.h5\"\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    monitor=\"val_kid\",\n",
        "    mode=\"min\",\n",
        "    save_best_only=True,\n",
        ")\n",
        "\n",
        "# calculate mean and variance of training dataset for normalization\n",
        "model.normalizer.adapt(train_dataset)\n",
        "\n",
        "# Build model explicitly before training to avoid \"not yet built\" error\n",
        "dummy_images = tf.zeros((1, image_size, image_size, 3))\n",
        "dummy_times = tf.zeros((1, 1, 1, 1))\n",
        "_ = model.network([dummy_images, dummy_times])  # build inner UNet\n",
        "_ = model.ema_network([dummy_images, dummy_times])  # build EMA network\n",
        "model.built = True\n",
        "print(\"Model built.\")\n",
        "\n",
        "# run training and plot generated images periodically\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    epochs=num_epochs,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[\n",
        "        keras.callbacks.LambdaCallback(on_epoch_end=model.plot_images),\n",
        "        checkpoint_callback,\n",
        "        metrics_logger,\n",
        "    ],\n",
        ")\n"
      ],
      "metadata": {
        "id": "LcT21pedVh2b",
        "outputId": "3944eecb-ac79-42ac-b424-7fdf68110421",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model built.\n",
            "Epoch 1/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.4388 - n_loss: 0.1576\n",
            "Sampling Time: 10.076578617095947\n",
            "\n",
            "Sampling Time: 2.204751968383789\n",
            "Saved generated images to generated_images/epoch_000_20251130-061710.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 132.91s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 114ms/step - i_loss: 0.4385 - n_loss: 0.1575 - val_i_loss: 2.2829 - val_kid: 0.5908 - val_n_loss: 0.7369\n",
            "Epoch 2/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.1259 - n_loss: 0.0573\n",
            "Sampling Time: 2.2101516723632812\n",
            "Saved generated images to generated_images/epoch_001_20251130-061818.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 67.51s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 72ms/step - i_loss: 0.1259 - n_loss: 0.0573 - val_i_loss: 1.4046 - val_kid: 0.5471 - val_n_loss: 0.4895\n",
            "Epoch 3/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.1029 - n_loss: 0.0470\n",
            "Sampling Time: 2.1972014904022217\n",
            "Saved generated images to generated_images/epoch_002_20251130-061925.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 67.19s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 72ms/step - i_loss: 0.1029 - n_loss: 0.0470 - val_i_loss: 0.7029 - val_kid: 0.4464 - val_n_loss: 0.2564\n",
            "Epoch 4/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0967 - n_loss: 0.0430\n",
            "Sampling Time: 2.190682888031006\n",
            "Saved generated images to generated_images/epoch_003_20251130-062032.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 67.46s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 72ms/step - i_loss: 0.0967 - n_loss: 0.0430 - val_i_loss: 0.3301 - val_kid: 0.2223 - val_n_loss: 0.1237\n",
            "Epoch 5/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0887 - n_loss: 0.0401\n",
            "Sampling Time: 2.2014873027801514\n",
            "Saved generated images to generated_images/epoch_004_20251130-062140.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 67.47s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 72ms/step - i_loss: 0.0887 - n_loss: 0.0401 - val_i_loss: 0.1592 - val_kid: 0.2047 - val_n_loss: 0.0648\n",
            "Epoch 6/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0843 - n_loss: 0.0378\n",
            "Sampling Time: 2.1768760681152344\n",
            "Saved generated images to generated_images/epoch_005_20251130-062247.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 66.80s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 71ms/step - i_loss: 0.0843 - n_loss: 0.0378 - val_i_loss: 0.0976 - val_kid: 0.1152 - val_n_loss: 0.0408\n",
            "Epoch 7/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0806 - n_loss: 0.0364\n",
            "Sampling Time: 2.204885482788086\n",
            "Saved generated images to generated_images/epoch_006_20251130-062353.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 66.53s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 71ms/step - i_loss: 0.0806 - n_loss: 0.0364 - val_i_loss: 0.0729 - val_kid: 0.0448 - val_n_loss: 0.0324\n",
            "Epoch 8/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0822 - n_loss: 0.0364\n",
            "Sampling Time: 2.176755428314209\n",
            "Saved generated images to generated_images/epoch_007_20251130-062501.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 67.21s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 72ms/step - i_loss: 0.0822 - n_loss: 0.0364 - val_i_loss: 0.0648 - val_kid: 0.0260 - val_n_loss: 0.0292\n",
            "Epoch 9/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0752 - n_loss: 0.0343\n",
            "Sampling Time: 2.1924076080322266\n",
            "Saved generated images to generated_images/epoch_008_20251130-062608.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 67.01s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 72ms/step - i_loss: 0.0752 - n_loss: 0.0343 - val_i_loss: 0.0616 - val_kid: 0.0191 - val_n_loss: 0.0278\n",
            "Epoch 10/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0736 - n_loss: 0.0331\n",
            "Sampling Time: 2.1837825775146484\n",
            "Saved generated images to generated_images/epoch_009_20251130-062715.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 67.05s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 72ms/step - i_loss: 0.0736 - n_loss: 0.0331 - val_i_loss: 0.0604 - val_kid: 0.0162 - val_n_loss: 0.0268\n",
            "Epoch 11/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0733 - n_loss: 0.0326\n",
            "Sampling Time: 2.1839439868927\n",
            "Saved generated images to generated_images/epoch_010_20251130-062822.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 66.12s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 71ms/step - i_loss: 0.0733 - n_loss: 0.0326 - val_i_loss: 0.0595 - val_kid: 0.0177 - val_n_loss: 0.0262\n",
            "Epoch 12/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0714 - n_loss: 0.0321\n",
            "Sampling Time: 2.19446063041687\n",
            "Saved generated images to generated_images/epoch_011_20251130-062928.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 66.06s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 71ms/step - i_loss: 0.0714 - n_loss: 0.0321 - val_i_loss: 0.0567 - val_kid: 0.0183 - val_n_loss: 0.0259\n",
            "Epoch 13/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0709 - n_loss: 0.0322\n",
            "Sampling Time: 2.1749279499053955\n",
            "Saved generated images to generated_images/epoch_012_20251130-063035.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 66.83s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 71ms/step - i_loss: 0.0709 - n_loss: 0.0322 - val_i_loss: 0.0562 - val_kid: 0.0184 - val_n_loss: 0.0254\n",
            "Epoch 14/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0718 - n_loss: 0.0320\n",
            "Sampling Time: 2.1810922622680664\n",
            "Saved generated images to generated_images/epoch_013_20251130-063141.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 66.06s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 71ms/step - i_loss: 0.0718 - n_loss: 0.0320 - val_i_loss: 0.0549 - val_kid: 0.0192 - val_n_loss: 0.0253\n",
            "Epoch 15/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0702 - n_loss: 0.0312\n",
            "Sampling Time: 2.1879992485046387\n",
            "Saved generated images to generated_images/epoch_014_20251130-063246.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.40s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 70ms/step - i_loss: 0.0702 - n_loss: 0.0312 - val_i_loss: 0.0543 - val_kid: 0.0193 - val_n_loss: 0.0250\n",
            "Epoch 16/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0684 - n_loss: 0.0309\n",
            "Sampling Time: 2.1929800510406494\n",
            "Saved generated images to generated_images/epoch_015_20251130-063352.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.75s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0684 - n_loss: 0.0309 - val_i_loss: 0.0550 - val_kid: 0.0186 - val_n_loss: 0.0248\n",
            "Epoch 17/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0694 - n_loss: 0.0307\n",
            "Sampling Time: 2.22171688079834\n",
            "Saved generated images to generated_images/epoch_016_20251130-063458.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.98s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0694 - n_loss: 0.0307 - val_i_loss: 0.0535 - val_kid: 0.0182 - val_n_loss: 0.0247\n",
            "Epoch 18/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0683 - n_loss: 0.0305\n",
            "Sampling Time: 2.1983978748321533\n",
            "Saved generated images to generated_images/epoch_017_20251130-063604.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 66.20s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 71ms/step - i_loss: 0.0683 - n_loss: 0.0305 - val_i_loss: 0.0544 - val_kid: 0.0178 - val_n_loss: 0.0243\n",
            "Epoch 19/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0669 - n_loss: 0.0299\n",
            "Sampling Time: 2.1945130825042725\n",
            "Saved generated images to generated_images/epoch_018_20251130-063710.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 66.08s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 71ms/step - i_loss: 0.0669 - n_loss: 0.0299 - val_i_loss: 0.0536 - val_kid: 0.0182 - val_n_loss: 0.0243\n",
            "Epoch 20/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0662 - n_loss: 0.0296\n",
            "Sampling Time: 2.1731297969818115\n",
            "Saved generated images to generated_images/epoch_019_20251130-063816.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.89s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0662 - n_loss: 0.0296 - val_i_loss: 0.0522 - val_kid: 0.0190 - val_n_loss: 0.0242\n",
            "Epoch 21/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0664 - n_loss: 0.0298\n",
            "Sampling Time: 2.212190628051758\n",
            "Saved generated images to generated_images/epoch_020_20251130-063922.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.72s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0664 - n_loss: 0.0298 - val_i_loss: 0.0534 - val_kid: 0.0216 - val_n_loss: 0.0241\n",
            "Epoch 22/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0659 - n_loss: 0.0296\n",
            "Sampling Time: 2.1942412853240967\n",
            "Saved generated images to generated_images/epoch_021_20251130-064028.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.88s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0659 - n_loss: 0.0296 - val_i_loss: 0.0516 - val_kid: 0.0204 - val_n_loss: 0.0239\n",
            "Epoch 23/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0657 - n_loss: 0.0292\n",
            "Sampling Time: 2.1956379413604736\n",
            "Saved generated images to generated_images/epoch_022_20251130-064134.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.83s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0657 - n_loss: 0.0292 - val_i_loss: 0.0530 - val_kid: 0.0202 - val_n_loss: 0.0238\n",
            "Epoch 24/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0666 - n_loss: 0.0292\n",
            "Sampling Time: 2.1807658672332764\n",
            "Saved generated images to generated_images/epoch_023_20251130-064240.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.82s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0666 - n_loss: 0.0292 - val_i_loss: 0.0516 - val_kid: 0.0204 - val_n_loss: 0.0237\n",
            "Epoch 25/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0653 - n_loss: 0.0291\n",
            "Sampling Time: 2.1730523109436035\n",
            "Saved generated images to generated_images/epoch_024_20251130-064345.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.66s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0653 - n_loss: 0.0291 - val_i_loss: 0.0508 - val_kid: 0.0210 - val_n_loss: 0.0237\n",
            "Epoch 26/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0656 - n_loss: 0.0292\n",
            "Sampling Time: 2.174736976623535\n",
            "Saved generated images to generated_images/epoch_025_20251130-064451.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.42s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 70ms/step - i_loss: 0.0656 - n_loss: 0.0292 - val_i_loss: 0.0511 - val_kid: 0.0195 - val_n_loss: 0.0235\n",
            "Epoch 27/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0656 - n_loss: 0.0287\n",
            "Sampling Time: 2.1930179595947266\n",
            "Saved generated images to generated_images/epoch_026_20251130-064556.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.42s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 70ms/step - i_loss: 0.0656 - n_loss: 0.0287 - val_i_loss: 0.0520 - val_kid: 0.0207 - val_n_loss: 0.0233\n",
            "Epoch 28/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0647 - n_loss: 0.0284\n",
            "Sampling Time: 2.2174720764160156\n",
            "Saved generated images to generated_images/epoch_027_20251130-064702.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 66.00s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0647 - n_loss: 0.0284 - val_i_loss: 0.0507 - val_kid: 0.0201 - val_n_loss: 0.0236\n",
            "Epoch 29/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - i_loss: 0.0633 - n_loss: 0.0283\n",
            "Sampling Time: 2.198376178741455\n",
            "Saved generated images to generated_images/epoch_028_20251130-064809.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 67.10s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 72ms/step - i_loss: 0.0633 - n_loss: 0.0283 - val_i_loss: 0.0507 - val_kid: 0.0204 - val_n_loss: 0.0233\n",
            "Epoch 30/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0647 - n_loss: 0.0285\n",
            "Sampling Time: 2.1838138103485107\n",
            "Saved generated images to generated_images/epoch_029_20251130-064915.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.79s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0647 - n_loss: 0.0285 - val_i_loss: 0.0513 - val_kid: 0.0213 - val_n_loss: 0.0233\n",
            "Epoch 31/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0647 - n_loss: 0.0285\n",
            "Sampling Time: 2.1933393478393555\n",
            "Saved generated images to generated_images/epoch_030_20251130-065021.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.73s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0647 - n_loss: 0.0285 - val_i_loss: 0.0516 - val_kid: 0.0202 - val_n_loss: 0.0231\n",
            "Epoch 32/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0632 - n_loss: 0.0284\n",
            "Sampling Time: 2.1810615062713623\n",
            "Saved generated images to generated_images/epoch_031_20251130-065126.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.77s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0632 - n_loss: 0.0284 - val_i_loss: 0.0505 - val_kid: 0.0210 - val_n_loss: 0.0232\n",
            "Epoch 33/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0629 - n_loss: 0.0282\n",
            "Sampling Time: 2.195133924484253\n",
            "Saved generated images to generated_images/epoch_032_20251130-065232.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.46s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 70ms/step - i_loss: 0.0629 - n_loss: 0.0282 - val_i_loss: 0.0503 - val_kid: 0.0193 - val_n_loss: 0.0232\n",
            "Epoch 34/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0641 - n_loss: 0.0280\n",
            "Sampling Time: 2.2025904655456543\n",
            "Saved generated images to generated_images/epoch_033_20251130-065338.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.62s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0640 - n_loss: 0.0280 - val_i_loss: 0.0516 - val_kid: 0.0199 - val_n_loss: 0.0230\n",
            "Epoch 35/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0631 - n_loss: 0.0279\n",
            "Sampling Time: 2.1787798404693604\n",
            "Saved generated images to generated_images/epoch_034_20251130-065443.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.81s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0631 - n_loss: 0.0279 - val_i_loss: 0.0504 - val_kid: 0.0189 - val_n_loss: 0.0231\n",
            "Epoch 36/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0620 - n_loss: 0.0277\n",
            "Sampling Time: 2.1790060997009277\n",
            "Saved generated images to generated_images/epoch_035_20251130-065549.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.57s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0620 - n_loss: 0.0277 - val_i_loss: 0.0510 - val_kid: 0.0209 - val_n_loss: 0.0228\n",
            "Epoch 37/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0621 - n_loss: 0.0276\n",
            "Sampling Time: 2.2073428630828857\n",
            "Saved generated images to generated_images/epoch_036_20251130-065655.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 66.04s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0621 - n_loss: 0.0276 - val_i_loss: 0.0508 - val_kid: 0.0207 - val_n_loss: 0.0228\n",
            "Epoch 38/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0610 - n_loss: 0.0276\n",
            "Sampling Time: 2.1908938884735107\n",
            "Saved generated images to generated_images/epoch_037_20251130-065801.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.93s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0610 - n_loss: 0.0276 - val_i_loss: 0.0512 - val_kid: 0.0207 - val_n_loss: 0.0228\n",
            "Epoch 39/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0609 - n_loss: 0.0276\n",
            "Sampling Time: 2.169417381286621\n",
            "Saved generated images to generated_images/epoch_038_20251130-065907.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.67s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0609 - n_loss: 0.0276 - val_i_loss: 0.0507 - val_kid: 0.0198 - val_n_loss: 0.0226\n",
            "Epoch 40/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0630 - n_loss: 0.0278\n",
            "Sampling Time: 2.1829028129577637\n",
            "Saved generated images to generated_images/epoch_039_20251130-070013.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.94s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0630 - n_loss: 0.0278 - val_i_loss: 0.0509 - val_kid: 0.0171 - val_n_loss: 0.0226\n",
            "Epoch 41/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0623 - n_loss: 0.0272\n",
            "Sampling Time: 2.18511700630188\n",
            "Saved generated images to generated_images/epoch_040_20251130-070118.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.28s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 70ms/step - i_loss: 0.0623 - n_loss: 0.0272 - val_i_loss: 0.0519 - val_kid: 0.0206 - val_n_loss: 0.0225\n",
            "Epoch 42/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0621 - n_loss: 0.0274\n",
            "Sampling Time: 2.1728732585906982\n",
            "Saved generated images to generated_images/epoch_041_20251130-070224.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.66s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0621 - n_loss: 0.0274 - val_i_loss: 0.0507 - val_kid: 0.0207 - val_n_loss: 0.0226\n",
            "Epoch 43/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0616 - n_loss: 0.0273\n",
            "Sampling Time: 2.1720621585845947\n",
            "Saved generated images to generated_images/epoch_042_20251130-070329.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.94s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0616 - n_loss: 0.0273 - val_i_loss: 0.0504 - val_kid: 0.0198 - val_n_loss: 0.0227\n",
            "Epoch 44/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0629 - n_loss: 0.0273\n",
            "Sampling Time: 2.1854355335235596\n",
            "Saved generated images to generated_images/epoch_043_20251130-070435.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.96s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0629 - n_loss: 0.0273 - val_i_loss: 0.0503 - val_kid: 0.0203 - val_n_loss: 0.0225\n",
            "Epoch 45/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0618 - n_loss: 0.0274\n",
            "Sampling Time: 2.211700201034546\n",
            "Saved generated images to generated_images/epoch_044_20251130-070541.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.97s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0618 - n_loss: 0.0274 - val_i_loss: 0.0506 - val_kid: 0.0187 - val_n_loss: 0.0225\n",
            "Epoch 46/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0604 - n_loss: 0.0271\n",
            "Sampling Time: 2.1967482566833496\n",
            "Saved generated images to generated_images/epoch_045_20251130-070647.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.77s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0604 - n_loss: 0.0271 - val_i_loss: 0.0500 - val_kid: 0.0203 - val_n_loss: 0.0226\n",
            "Epoch 47/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0603 - n_loss: 0.0271\n",
            "Sampling Time: 2.1892170906066895\n",
            "Saved generated images to generated_images/epoch_046_20251130-070753.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.89s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0603 - n_loss: 0.0271 - val_i_loss: 0.0487 - val_kid: 0.0191 - val_n_loss: 0.0225\n",
            "Epoch 48/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0606 - n_loss: 0.0273\n",
            "Sampling Time: 2.21809720993042\n",
            "Saved generated images to generated_images/epoch_047_20251130-070859.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.89s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0606 - n_loss: 0.0273 - val_i_loss: 0.0495 - val_kid: 0.0197 - val_n_loss: 0.0225\n",
            "Epoch 49/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0610 - n_loss: 0.0273\n",
            "Sampling Time: 2.1889395713806152\n",
            "Saved generated images to generated_images/epoch_048_20251130-071006.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 66.65s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 71ms/step - i_loss: 0.0610 - n_loss: 0.0273 - val_i_loss: 0.0505 - val_kid: 0.0192 - val_n_loss: 0.0224\n",
            "Epoch 50/50\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - i_loss: 0.0601 - n_loss: 0.0268\n",
            "Sampling Time: 2.167876958847046\n",
            "Saved generated images to generated_images/epoch_049_20251130-071112.png\n",
            "Metrics saved to training_metrics.csv (Epoch time: 65.84s)\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 70ms/step - i_loss: 0.0601 - n_loss: 0.0268 - val_i_loss: 0.0491 - val_kid: 0.0173 - val_n_loss: 0.0225\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7de09d2e3500>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Replace 'my_folder' with the actual name of the folder you want to download\n",
        "# The first argument is the name of the zip file to be created (without extension)\n",
        "# The second argument is the format ('zip', 'tar', etc.)\n",
        "# The third argument is the path to the folder you want to compress\n",
        "shutil.make_archive(\"my_folder_archive\", 'zip', \"generated_images\")"
      ],
      "metadata": {
        "id": "_UAIlJYUWa6z",
        "outputId": "2c2977c1-2b9a-4601-deea-f7b7a50a5a87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/my_folder_archive.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'my_folder_archive.zip' with the name of the zip file you created\n",
        "files.download(\"my_folder_archive.zip\")"
      ],
      "metadata": {
        "id": "l_reaimMnMao",
        "outputId": "a5d0bf03-985d-4e5f-c9e1-656423476941",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_be7b92cd-a4f3-44ea-a9c1-1b3ca55354c0\", \"my_folder_archive.zip\", 14647550)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f0uDtqtlnNMq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}