{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPuuN7d_iAWh"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aNKaMwDLhtmQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import imageio\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import make_grid\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import grad as torch_grad\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pn9A9j9h3Qv"
      },
      "source": [
        "# Models.py\n",
        "## Generator and Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "w3W5Kn71g2o8"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, img_size, latent_dim, dim):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_size = img_size\n",
        "        self.feature_sizes = (self.img_size[0] // 16, self.img_size[1] // 16)\n",
        "\n",
        "        self.latent_to_features = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 8 * dim * self.feature_sizes[0] * self.feature_sizes[1]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.features_to_image = nn.Sequential(\n",
        "            nn.ConvTranspose2d(8 * dim, 4 * dim, 4, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(4 * dim),\n",
        "            nn.ConvTranspose2d(4 * dim, 2 * dim, 4, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(2 * dim),\n",
        "            nn.ConvTranspose2d(2 * dim, dim, 4, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(dim),\n",
        "            nn.ConvTranspose2d(dim, self.img_size[2], 4, 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        # Map latent into appropriate size for transposed convolutions\n",
        "        x = self.latent_to_features(input_data)\n",
        "        # Reshape\n",
        "        x = x.view(-1, 8 * self.dim, self.feature_sizes[0], self.feature_sizes[1])\n",
        "        # Return generated image\n",
        "        return self.features_to_image(x)\n",
        "\n",
        "    def sample_latent(self, num_samples):\n",
        "        return torch.randn((num_samples, self.latent_dim))\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_size, dim):\n",
        "        \"\"\"\n",
        "        img_size : (int, int, int)\n",
        "            Height and width must be powers of 2.  E.g. (32, 32, 1) or\n",
        "            (64, 128, 3). Last number indicates number of channels, e.g. 1 for\n",
        "            grayscale or 3 for RGB\n",
        "        \"\"\"\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.img_size = img_size\n",
        "\n",
        "        self.image_to_features = nn.Sequential(\n",
        "            nn.Conv2d(self.img_size[2], dim, 4, 2, 1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(dim, 2 * dim, 4, 2, 1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(2 * dim, 4 * dim, 4, 2, 1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(4 * dim, 8 * dim, 4, 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # 4 convolutions of stride 2, i.e. halving of size everytime\n",
        "        # So output size will be 8 * (img_size / 2 ^ 4) * (img_size / 2 ^ 4)\n",
        "        output_size = 8 * dim * (img_size[0] // 16) * (img_size[1] // 16)\n",
        "        self.features_to_prob = nn.Sequential(\n",
        "            nn.Linear(output_size, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        batch_size = input_data.size()[0]\n",
        "        x = self.image_to_features(input_data)\n",
        "        x = x.view(batch_size, -1)\n",
        "        return self.features_to_prob(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeVU3p7siMm4"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Pnf97xQ3hYoo"
      },
      "outputs": [],
      "source": [
        "class Trainer():\n",
        "    def __init__(self, generator, discriminator, gen_optimizer, dis_optimizer,\n",
        "                 gp_weight=10, critic_iterations=5, print_every=50,\n",
        "                 use_cuda=False):\n",
        "        self.G = generator\n",
        "        self.G_opt = gen_optimizer\n",
        "        self.D = discriminator\n",
        "        self.D_opt = dis_optimizer\n",
        "        self.losses = {'G': [], 'D': [], 'GP': [], 'gradient_norm': []}\n",
        "        self.num_steps = 0\n",
        "        self.use_cuda = use_cuda\n",
        "        self.gp_weight = gp_weight\n",
        "        self.critic_iterations = critic_iterations\n",
        "        self.print_every = print_every\n",
        "\n",
        "        if self.use_cuda:\n",
        "            self.G.cuda()\n",
        "            self.D.cuda()\n",
        "\n",
        "    def _critic_train_iteration(self, data):\n",
        "        \"\"\" \"\"\"\n",
        "        # Get generated data\n",
        "        batch_size = data.size()[0]\n",
        "        generated_data = self.sample_generator(batch_size)\n",
        "\n",
        "        # Calculate probabilities on real and generated data\n",
        "        data = Variable(data)\n",
        "        if self.use_cuda:\n",
        "            data = data.cuda()\n",
        "        d_real = self.D(data)\n",
        "        d_generated = self.D(generated_data)\n",
        "\n",
        "        # Get gradient penalty\n",
        "        gradient_penalty = self._gradient_penalty(data, generated_data)\n",
        "        self.losses['GP'].append(gradient_penalty.item())\n",
        "\n",
        "        # Create total loss and optimize\n",
        "        self.D_opt.zero_grad()\n",
        "        d_loss = d_generated.mean() - d_real.mean() + gradient_penalty\n",
        "        d_loss.backward()\n",
        "\n",
        "        self.D_opt.step()\n",
        "\n",
        "        # Record loss\n",
        "        self.losses['D'].append(d_loss.item())\n",
        "\n",
        "    def _generator_train_iteration(self, data):\n",
        "        \"\"\" \"\"\"\n",
        "        self.G_opt.zero_grad()\n",
        "\n",
        "        # Get generated data\n",
        "        batch_size = data.size()[0]\n",
        "        generated_data = self.sample_generator(batch_size)\n",
        "\n",
        "        # Calculate loss and optimize\n",
        "        d_generated = self.D(generated_data)\n",
        "        g_loss = - d_generated.mean()\n",
        "        g_loss.backward()\n",
        "        self.G_opt.step()\n",
        "\n",
        "        # Record loss\n",
        "        self.losses['G'].append(g_loss.item())\n",
        "\n",
        "    def _gradient_penalty(self, real_data, generated_data):\n",
        "        batch_size = real_data.size()[0]\n",
        "\n",
        "        # Calculate interpolation\n",
        "        alpha = torch.rand(batch_size, 1, 1, 1)\n",
        "        alpha = alpha.expand_as(real_data)\n",
        "        if self.use_cuda:\n",
        "            alpha = alpha.cuda()\n",
        "        interpolated = alpha * real_data.data + (1 - alpha) * generated_data.data\n",
        "        interpolated = Variable(interpolated, requires_grad=True)\n",
        "        if self.use_cuda:\n",
        "            interpolated = interpolated.cuda()\n",
        "\n",
        "        # Calculate probability of interpolated examples\n",
        "        prob_interpolated = self.D(interpolated)\n",
        "\n",
        "        # Calculate gradients of probabilities with respect to examples\n",
        "        gradients = torch_grad(outputs=prob_interpolated, inputs=interpolated,\n",
        "                               grad_outputs=torch.ones(prob_interpolated.size()).cuda() if self.use_cuda else torch.ones(\n",
        "                               prob_interpolated.size()),\n",
        "                               create_graph=True, retain_graph=True)[0]\n",
        "\n",
        "        # Gradients have shape (batch_size, num_channels, img_width, img_height),\n",
        "        # so flatten to easily take norm per example in batch\n",
        "        gradients = gradients.view(batch_size, -1)\n",
        "        self.losses['gradient_norm'].append(gradients.norm(2, dim=1).mean().item())\n",
        "\n",
        "        # Derivatives of the gradient close to 0 can cause problems because of\n",
        "        # the square root, so manually calculate norm and add epsilon\n",
        "        gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
        "\n",
        "        # Return gradient penalty\n",
        "        return self.gp_weight * ((gradients_norm - 1) ** 2).mean()\n",
        "\n",
        "    def _train_epoch(self, data_loader):\n",
        "        for i, data in enumerate(data_loader):\n",
        "            self.num_steps += 1\n",
        "            self._critic_train_iteration(data[0])\n",
        "            # Only update generator every |critic_iterations| iterations\n",
        "            if self.num_steps % self.critic_iterations == 0:\n",
        "                self._generator_train_iteration(data[0])\n",
        "\n",
        "            if i % self.print_every == 0:\n",
        "                print(\"Iteration {}\".format(i + 1))\n",
        "                print(\"D: {}\".format(self.losses['D'][-1]))\n",
        "                print(\"GP: {}\".format(self.losses['GP'][-1]))\n",
        "                print(\"Gradient norm: {}\".format(self.losses['gradient_norm'][-1]))\n",
        "                if self.num_steps > self.critic_iterations:\n",
        "                    print(\"G: {}\".format(self.losses['G'][-1]))\n",
        "\n",
        "    def train(self, data_loader, epochs, save_training_gif=True):\n",
        "        if save_training_gif:\n",
        "            # Fix latents to see how image generation improves during training\n",
        "            fixed_latents = Variable(self.G.sample_latent(64))\n",
        "            if self.use_cuda:\n",
        "                fixed_latents = fixed_latents.cuda()\n",
        "            training_progress_images = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(\"\\nEpoch {}\".format(epoch + 1))\n",
        "            self._train_epoch(data_loader)\n",
        "\n",
        "            if save_training_gif:\n",
        "                # # Generate batch of images and convert to grid\n",
        "                # img_grid = make_grid(self.G(fixed_latents).cpu().data)\n",
        "                # # Convert to numpy and transpose axes to fit imageio convention\n",
        "                # # i.e. (width, height, channels)\n",
        "                # img_grid = np.transpose(img_grid.numpy(), (1, 2, 0))\n",
        "                # # Add image grid to training progress\n",
        "                # training_progress_images.append(img_grid)\n",
        "                grid_t = make_grid(\n",
        "                    self.G(fixed_latents).detach().cpu(),  # detach; no .data\n",
        "                    nrow=8,\n",
        "                    normalize=True,        # scales to [0,1] using min/max per image\n",
        "                    value_range=(0, 1),    # if your generator already outputs [0,1], still OK\n",
        "                    padding=2\n",
        "                )\n",
        "\n",
        "                # 2) Convert CHW -> HWC and to uint8 [0,255]\n",
        "                img_grid = (grid_t.permute(1, 2, 0).numpy() * 255.0).clip(0, 255).astype(np.uint8)\n",
        "\n",
        "                # 3) Append frame\n",
        "                training_progress_images.append(img_grid)\n",
        "\n",
        "        if save_training_gif:\n",
        "            imageio.mimsave('./training_{}_epochs.gif'.format(epochs),\n",
        "                            training_progress_images)\n",
        "\n",
        "    def sample_generator(self, num_samples):\n",
        "        latent_samples = Variable(self.G.sample_latent(num_samples))\n",
        "        if self.use_cuda:\n",
        "            latent_samples = latent_samples.cuda()\n",
        "        generated_data = self.G(latent_samples)\n",
        "        return generated_data\n",
        "\n",
        "    def sample(self, num_samples):\n",
        "        generated_data = self.sample_generator(num_samples)\n",
        "        # Remove color channel\n",
        "        return generated_data.data.cpu().numpy()[:, 0, :, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71CgegNhikBD"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rWAN7nZkig2u"
      },
      "outputs": [],
      "source": [
        "def get_mnist_dataloaders(batch_size=128):\n",
        "    \"\"\"MNIST dataloader with (32, 32) sized images.\"\"\"\n",
        "    # Resize images so they are a power of 2\n",
        "    all_transforms = transforms.Compose([\n",
        "        transforms.Resize(32),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    # Get train and test data\n",
        "    train_data = datasets.MNIST('../data', train=True, download=True,\n",
        "                                transform=all_transforms)\n",
        "    test_data = datasets.MNIST('../data', train=False,\n",
        "                               transform=all_transforms)\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def get_fashion_mnist_dataloaders(batch_size=128):\n",
        "    \"\"\"Fashion MNIST dataloader with (32, 32) sized images.\"\"\"\n",
        "    # Resize images so they are a power of 2\n",
        "    all_transforms = transforms.Compose([\n",
        "        transforms.Resize(32),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    # Get train and test data\n",
        "    train_data = datasets.FashionMNIST('../fashion_data', train=True, download=True,\n",
        "                                       transform=all_transforms)\n",
        "    test_data = datasets.FashionMNIST('../fashion_data', train=False,\n",
        "                                      transform=all_transforms)\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def get_lsun_dataloader(path_to_data='../lsun', dataset='bedroom_train',\n",
        "                        batch_size=64):\n",
        "    \"\"\"LSUN dataloader with (128, 128) sized images.\n",
        "\n",
        "    path_to_data : str\n",
        "        One of 'bedroom_val' or 'bedroom_train'\n",
        "    \"\"\"\n",
        "    # Compose transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(128),\n",
        "        transforms.CenterCrop(128),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    # Get dataset\n",
        "    lsun_dset = datasets.LSUN(db_path=path_to_data, classes=[dataset],\n",
        "                              transform=transform)\n",
        "\n",
        "    # Create dataloader\n",
        "    return DataLoader(lsun_dset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtwNjAvyiv_L"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D-tHvtYixEd",
        "outputId": "d3fcbf41-bc7d-47ed-c76e-a6684e8fdc92"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 3.53MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 393kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.44MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 1.79MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generator(\n",
            "  (latent_to_features): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "  )\n",
            "  (features_to_image): Sequential(\n",
            "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (4): ReLU()\n",
            "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (7): ReLU()\n",
            "    (8): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ConvTranspose2d(16, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (10): Sigmoid()\n",
            "  )\n",
            ")\n",
            "Discriminator(\n",
            "  (image_to_features): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (1): LeakyReLU(negative_slope=0.2)\n",
            "    (2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (3): LeakyReLU(negative_slope=0.2)\n",
            "    (4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (5): LeakyReLU(negative_slope=0.2)\n",
            "    (6): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (7): Sigmoid()\n",
            "  )\n",
            "  (features_to_prob): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=1, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            ")\n",
            "\n",
            "Epoch 1\n",
            "Iteration 1\n",
            "D: 9.98251724243164\n",
            "GP: 9.982575416564941\n",
            "Gradient norm: 0.0008715791627764702\n",
            "Iteration 51\n",
            "D: 9.592475891113281\n",
            "GP: 9.587306022644043\n",
            "Gradient norm: 0.020852364599704742\n",
            "G: -0.482477605342865\n",
            "Iteration 101\n",
            "D: 7.536064624786377\n",
            "GP: 7.452398300170898\n",
            "Gradient norm: 0.13677987456321716\n",
            "G: -0.5432210564613342\n",
            "Iteration 151\n",
            "D: 3.77089524269104\n",
            "GP: 3.7318851947784424\n",
            "Gradient norm: 0.39068707823753357\n",
            "G: -0.5989469885826111\n",
            "Iteration 201\n",
            "D: 1.502359390258789\n",
            "GP: 1.5628266334533691\n",
            "Gradient norm: 0.6198951005935669\n",
            "G: -0.5801446437835693\n",
            "Iteration 251\n",
            "D: 1.0159292221069336\n",
            "GP: 1.040184497833252\n",
            "Gradient norm: 0.7013393044471741\n",
            "G: -0.6196925640106201\n",
            "Iteration 301\n",
            "D: 0.6742479801177979\n",
            "GP: 0.8437418937683105\n",
            "Gradient norm: 0.7834991216659546\n",
            "G: -0.5492355823516846\n",
            "Iteration 351\n",
            "D: 1.0415396690368652\n",
            "GP: 1.170534610748291\n",
            "Gradient norm: 0.7255743741989136\n",
            "G: -0.574499249458313\n",
            "Iteration 401\n",
            "D: 0.6443365216255188\n",
            "GP: 0.8231523036956787\n",
            "Gradient norm: 0.8151678442955017\n",
            "G: -0.5577783584594727\n",
            "Iteration 451\n",
            "D: 0.5825249552726746\n",
            "GP: 0.7686352729797363\n",
            "Gradient norm: 0.830355167388916\n",
            "G: -0.5672138333320618\n",
            "Iteration 501\n",
            "D: 0.4389200210571289\n",
            "GP: 0.6463053822517395\n",
            "Gradient norm: 0.8317899107933044\n",
            "G: -0.5594362020492554\n",
            "Iteration 551\n",
            "D: 0.22865116596221924\n",
            "GP: 0.4414162039756775\n",
            "Gradient norm: 0.8765773773193359\n",
            "G: -0.5403425693511963\n",
            "Iteration 601\n",
            "D: 0.2229485809803009\n",
            "GP: 0.42155417799949646\n",
            "Gradient norm: 0.8717246651649475\n",
            "G: -0.5550720691680908\n",
            "Iteration 651\n",
            "D: 0.08416420221328735\n",
            "GP: 0.3353172540664673\n",
            "Gradient norm: 0.9006341099739075\n",
            "G: -0.48511528968811035\n",
            "Iteration 701\n",
            "D: 0.2372082769870758\n",
            "GP: 0.494415283203125\n",
            "Gradient norm: 0.873256504535675\n",
            "G: -0.4804658889770508\n",
            "Iteration 751\n",
            "D: 0.059248536825180054\n",
            "GP: 0.4165359139442444\n",
            "Gradient norm: 0.9024887084960938\n",
            "G: -0.4478384852409363\n",
            "Iteration 801\n",
            "D: -0.19324292242527008\n",
            "GP: 0.20456276834011078\n",
            "Gradient norm: 0.9679386615753174\n",
            "G: -0.43098926544189453\n",
            "Iteration 851\n",
            "D: -0.0961909294128418\n",
            "GP: 0.29106611013412476\n",
            "Gradient norm: 0.9183132648468018\n",
            "G: -0.4157435894012451\n",
            "Iteration 901\n",
            "D: -0.1376664638519287\n",
            "GP: 0.30287855863571167\n",
            "Gradient norm: 0.9762574434280396\n",
            "G: -0.3790631592273712\n",
            "\n",
            "Epoch 2\n",
            "Iteration 1\n",
            "D: -0.31075847148895264\n",
            "GP: 0.19835151731967926\n",
            "Gradient norm: 0.9653823375701904\n",
            "G: -0.3627091646194458\n",
            "Iteration 51\n",
            "D: -0.2820814251899719\n",
            "GP: 0.22695152461528778\n",
            "Gradient norm: 0.954436182975769\n",
            "G: -0.35053813457489014\n",
            "Iteration 101\n",
            "D: -0.3002053499221802\n",
            "GP: 0.21916456520557404\n",
            "Gradient norm: 0.9678534269332886\n",
            "G: -0.359982967376709\n",
            "Iteration 151\n",
            "D: -0.39304643869400024\n",
            "GP: 0.1353248804807663\n",
            "Gradient norm: 0.9784948825836182\n",
            "G: -0.325670063495636\n",
            "Iteration 201\n",
            "D: -0.270906537771225\n",
            "GP: 0.22822290658950806\n",
            "Gradient norm: 0.93059241771698\n",
            "G: -0.3096241056919098\n",
            "Iteration 251\n",
            "D: -0.41022056341171265\n",
            "GP: 0.1493573784828186\n",
            "Gradient norm: 0.991314172744751\n",
            "G: -0.28523364663124084\n",
            "Iteration 301\n",
            "D: -0.3900306820869446\n",
            "GP: 0.1735973209142685\n",
            "Gradient norm: 0.9523738622665405\n",
            "G: -0.30967262387275696\n",
            "Iteration 351\n",
            "D: -0.34298402070999146\n",
            "GP: 0.18932801485061646\n",
            "Gradient norm: 0.9543759822845459\n",
            "G: -0.34733226895332336\n",
            "Iteration 401\n",
            "D: -0.3408939838409424\n",
            "GP: 0.24901239573955536\n",
            "Gradient norm: 0.982589840888977\n",
            "G: -0.3100515604019165\n",
            "Iteration 451\n",
            "D: -0.373975545167923\n",
            "GP: 0.24023357033729553\n",
            "Gradient norm: 0.9472039937973022\n",
            "G: -0.29752081632614136\n",
            "Iteration 501\n",
            "D: -0.4302886724472046\n",
            "GP: 0.155791774392128\n",
            "Gradient norm: 0.9788132905960083\n",
            "G: -0.3257579803466797\n",
            "Iteration 551\n",
            "D: -0.4992161989212036\n",
            "GP: 0.09871745109558105\n",
            "Gradient norm: 0.977709949016571\n",
            "G: -0.29716092348098755\n",
            "Iteration 601\n",
            "D: -0.4319606423377991\n",
            "GP: 0.17942340672016144\n",
            "Gradient norm: 0.9619731903076172\n",
            "G: -0.304589182138443\n",
            "Iteration 651\n",
            "D: -0.4916832149028778\n",
            "GP: 0.1035325899720192\n",
            "Gradient norm: 0.9683610200881958\n",
            "G: -0.26478642225265503\n",
            "Iteration 701\n",
            "D: -0.4694875478744507\n",
            "GP: 0.12237703800201416\n",
            "Gradient norm: 0.9879623651504517\n",
            "G: -0.2733380198478699\n",
            "Iteration 751\n",
            "D: -0.41383934020996094\n",
            "GP: 0.18127548694610596\n",
            "Gradient norm: 0.9704138040542603\n",
            "G: -0.3055909276008606\n",
            "Iteration 801\n",
            "D: -0.3457688093185425\n",
            "GP: 0.2894834876060486\n",
            "Gradient norm: 0.9609746932983398\n",
            "G: -0.3058655858039856\n",
            "Iteration 851\n",
            "D: -0.37361520528793335\n",
            "GP: 0.23580341041088104\n",
            "Gradient norm: 0.9762469530105591\n",
            "G: -0.28544843196868896\n",
            "Iteration 901\n",
            "D: -0.41412538290023804\n",
            "GP: 0.19385068118572235\n",
            "Gradient norm: 0.9463208913803101\n",
            "G: -0.26921725273132324\n",
            "\n",
            "Epoch 3\n",
            "Iteration 1\n",
            "D: -0.4611811935901642\n",
            "GP: 0.20181307196617126\n",
            "Gradient norm: 0.9478445649147034\n",
            "G: -0.22937217354774475\n",
            "Iteration 51\n",
            "D: -0.5158581733703613\n",
            "GP: 0.11291541159152985\n",
            "Gradient norm: 0.9515165090560913\n",
            "G: -0.25001221895217896\n",
            "Iteration 101\n",
            "D: -0.5004463791847229\n",
            "GP: 0.16774655878543854\n",
            "Gradient norm: 0.9907774329185486\n",
            "G: -0.2326982319355011\n",
            "Iteration 151\n",
            "D: -0.5690937042236328\n",
            "GP: 0.07485240697860718\n",
            "Gradient norm: 0.9806104898452759\n",
            "G: -0.25840407609939575\n",
            "Iteration 201\n",
            "D: -0.5306432247161865\n",
            "GP: 0.1400374174118042\n",
            "Gradient norm: 0.973674476146698\n",
            "G: -0.24497798085212708\n",
            "Iteration 251\n",
            "D: -0.5632553100585938\n",
            "GP: 0.10211170464754105\n",
            "Gradient norm: 0.9721344709396362\n",
            "G: -0.24998784065246582\n",
            "Iteration 301\n",
            "D: -0.5347939133644104\n",
            "GP: 0.13423828780651093\n",
            "Gradient norm: 0.9583414793014526\n",
            "G: -0.25571733713150024\n",
            "Iteration 351\n",
            "D: -0.46419060230255127\n",
            "GP: 0.17811991274356842\n",
            "Gradient norm: 0.9886845350265503\n",
            "G: -0.23457300662994385\n",
            "Iteration 401\n",
            "D: -0.4322645962238312\n",
            "GP: 0.24142757058143616\n",
            "Gradient norm: 0.9990894794464111\n",
            "G: -0.22357811033725739\n",
            "Iteration 451\n",
            "D: -0.4974132776260376\n",
            "GP: 0.14191055297851562\n",
            "Gradient norm: 0.9911236763000488\n",
            "G: -0.26856479048728943\n",
            "Iteration 501\n",
            "D: -0.48613983392715454\n",
            "GP: 0.13120751082897186\n",
            "Gradient norm: 0.9723338484764099\n",
            "G: -0.2726942002773285\n"
          ]
        }
      ],
      "source": [
        "data_loader, _ = get_mnist_dataloaders(batch_size=64)\n",
        "img_size = (32, 32, 1)\n",
        "\n",
        "generator = Generator(img_size=img_size, latent_dim=100, dim=16)\n",
        "discriminator = Discriminator(img_size=img_size, dim=16)\n",
        "\n",
        "print(generator)\n",
        "print(discriminator)\n",
        "\n",
        "# Initialize optimizers\n",
        "lr = 1e-4\n",
        "betas = (.9, .99)\n",
        "G_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=betas)\n",
        "D_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=betas)\n",
        "\n",
        "# Train model\n",
        "epochs = 200\n",
        "trainer = Trainer(generator, discriminator, G_optimizer, D_optimizer,\n",
        "                  use_cuda=torch.cuda.is_available())\n",
        "trainer.train(data_loader, epochs, save_training_gif=True)\n",
        "\n",
        "# Save models\n",
        "name = 'mnist_model'\n",
        "torch.save(trainer.G.state_dict(), './gen_' + name + '.pt')\n",
        "torch.save(trainer.D.state_dict(), './dis_' + name + '.pt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
